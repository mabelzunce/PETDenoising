{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d511aa11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nb\n",
    "import SimpleITK as sitk\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import torch \n",
    "from torch import nn\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os, glob, cv2, sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c799642a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo\n",
    "\n",
    "class Unet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Unet,self).__init__()\n",
    "        \n",
    "        # Contract\n",
    "        self.layer1Down = nn.Sequential (\n",
    "            torch.nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.BatchNorm2d(32),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.BatchNorm2d(32),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.layer2Down = nn.Sequential (\n",
    "            torch.nn.Conv2d(32, 64, kernel_size = 3, stride=1, padding=1),\n",
    "            torch.nn.BatchNorm2d(64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(64, 64, kernel_size= 3, stride=1, padding=1),\n",
    "            torch.nn.BatchNorm2d(64),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.layer3Down = nn.Sequential (\n",
    "            torch.nn.Conv2d(64, 128, kernel_size = 3, stride=1, padding=1),\n",
    "            torch.nn.BatchNorm2d(128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(128, 128, kernel_size = 3, stride=1, padding=1),\n",
    "            torch.nn.BatchNorm2d(128),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.layer4Down = nn.Sequential (\n",
    "            torch.nn.Conv2d(128, 256, kernel_size = 3, stride=1, padding=1),\n",
    "            torch.nn.BatchNorm2d(256),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(256, 256, kernel_size = 3, stride=1, padding=1),\n",
    "            torch.nn.BatchNorm2d(256),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.layer5Down = nn.Sequential (\n",
    "            torch.nn.Conv2d(256, 512, kernel_size = 3, stride=1, padding=1),\n",
    "            torch.nn.BatchNorm2d(512),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(512, 512, kernel_size = 3, stride=1, padding=1),\n",
    "            torch.nn.BatchNorm2d(512),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.Pooling = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        #Expand\n",
    "        \n",
    "        self.layer5ConvTransposed = nn.Sequential ( \n",
    "            torch.nn.ConvTranspose2d(512, 256, kernel_size = 2, stride = 2, padding = 0)\n",
    "        )\n",
    "        \n",
    "        self.layer4Up = nn.Sequential (\n",
    "            torch.nn.Conv2d(512, 256, kernel_size = 3, stride=1, padding=1),\n",
    "            torch.nn.BatchNorm2d(256),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(256, 256, kernel_size = 3, stride=1, padding=1),\n",
    "            torch.nn.BatchNorm2d(256),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.layer4ConvTransposed = nn.Sequential ( \n",
    "            torch.nn.ConvTranspose2d(256, 128, kernel_size = 2, stride = 2, padding = 0)\n",
    "        )\n",
    "        \n",
    "        self.layer3Up = nn.Sequential (\n",
    "            torch.nn.Conv2d(256, 128, kernel_size = 3, stride=1, padding=1),\n",
    "            torch.nn.BatchNorm2d(128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(128, 128, kernel_size = 3, stride=1, padding=1),\n",
    "            torch.nn.BatchNorm2d(128),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.layer3ConvTransposed = nn.Sequential ( \n",
    "            torch.nn.ConvTranspose2d(128, 64, kernel_size = 2, stride = 2, padding = 0)\n",
    "        )\n",
    "        \n",
    "        self.layer2Up = nn.Sequential (\n",
    "            torch.nn.Conv2d(128, 64, kernel_size = 3, stride=1, padding=1),\n",
    "            torch.nn.BatchNorm2d(64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(64, 64, kernel_size = 3, stride=1, padding=1),\n",
    "            torch.nn.BatchNorm2d(64),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.layer2ConvTransposed = nn.Sequential ( \n",
    "            torch.nn.ConvTranspose2d(64, 32, kernel_size = 2, stride = 2, padding = 0)\n",
    "        )\n",
    "        \n",
    "        self.layer1Up = nn.Sequential (\n",
    "            torch.nn.Conv2d(64, 32, kernel_size = 3, stride=1, padding=1),\n",
    "            torch.nn.BatchNorm2d(32),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(32, 32, kernel_size = 3, stride=1, padding=1),\n",
    "            torch.nn.BatchNorm2d(32),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        # Down\n",
    "        conv1 = self.layer1Down(x)\n",
    "        maxPool1 = self.Pooling(conv1)\n",
    "        \n",
    "        \n",
    "        conv2 = self.layer2Down(maxPool1)\n",
    "        maxPool2 = self.Pooling(conv2)\n",
    "        \n",
    "        conv3 = self.layer3Down(maxPool2)\n",
    "        maxPool3 = self.Pooling(conv3)\n",
    "        \n",
    "        conv4 = self.layer4Down(maxPool3)\n",
    "        maxPool4 = self.Pooling(conv4)\n",
    "        \n",
    "        conv5 = self.layer5Down(maxPool4)\n",
    "        \n",
    "        # Up\n",
    "        layerConvTransposed = self.layer5ConvTransposed(conv5)\n",
    "        layer4UpData = torch.cat((layerConvTransposed,conv4), dim=1)\n",
    "        convTrans4 = self.layer4Up(layer4UpData)\n",
    "        \n",
    " \n",
    "        layerConvTransposed = self.layer4ConvTransposed(convTrans4)\n",
    "        layer3UpData = torch.cat((layerConvTransposed,conv3), dim = 1)\n",
    "        convTrans3 = self.layer3Up(layer3UpData)\n",
    "        \n",
    "        layerConvTransposed = self.layer3ConvTransposed(convTrans3)\n",
    "        layer2UpData = torch.cat((layerConvTransposed,conv2), dim = 1)\n",
    "        convTrans2 = self.layer2Up(layer2UpData)\n",
    "        \n",
    "        layerConvTransposed = self.layer2ConvTransposed(convTrans2)\n",
    "        layer1UpData = torch.cat((layerConvTransposed,conv1), dim = 1)\n",
    "        convTrans1 = self.layer1Up(layer1UpData)\n",
    "        \n",
    "        outNet = torch.nn.Conv2d(32,2, kernel_size = 1)(convTrans1)\n",
    "        return outNet\n",
    "\n",
    "unet = Unet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7e09d0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga de datos\n",
    "\n",
    "noisyDataSet1_nii = sitk.ReadImage('./noisyDataSet1.nii')\n",
    "img_noisyDataSet1 = sitk.GetArrayFromImage(noisyDataSet1_nii)\n",
    "\n",
    "noisyDataSet2_nii = sitk.ReadImage('./noisyDataSet2.nii')\n",
    "img_noisyDataSet2 = sitk.GetArrayFromImage(noisyDataSet2_nii)\n",
    "\n",
    "groundTruth_nii = sitk.ReadImage('./groundTruth.nii')\n",
    "img_groundTruth = sitk.GetArrayFromImage(groundTruth_nii)\n",
    "\n",
    "img_noisyDataSet1 =img_noisyDataSet1[:,44:300,44:300]\n",
    "img_noisyDataSet2 =img_noisyDataSet2[:,44:300,44:300]\n",
    "img_groundTruth =img_groundTruth[:,44:300,44:300]\n",
    "\n",
    "img_noisyDataSet1 = np.expand_dims(img_noisyDataSet1, axis=-1)\n",
    "img_noisyDataSet2 = np.expand_dims(img_noisyDataSet2, axis=-1)\n",
    "img_groundTruth = np.expand_dims(img_groundTruth, axis=-1)\n",
    "\n",
    "img_noisyDataSet1 = img_noisyDataSet1.astype(np.float32)\n",
    "img_noisyDataSet2 = img_noisyDataSet2.astype(np.float32)\n",
    "img_groundTruth = img_groundTruth.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9a3b3692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(unet.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "aa428ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conjunto de ENTRENAMIENTO, TESTEO y VALIDACION\n",
    "\n",
    "train_noisyImage,test_noisyImage,train_groundTruth,test_groundTruth = train_test_split(img_noisyDataSet1, img_groundTruth, test_size=0.2)\n",
    "\n",
    "valid_noisyImage = train_noisyImage[-5:,:,:,:]\n",
    "valid_groundTruth = train_groundTruth[-5:,:,:,:]\n",
    "\n",
    "train_noisyImage = train_noisyImage [:-5,:,:,:]\n",
    "train_groundTruth = train_groundTruth[:-5:,:,:,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ee89d445",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "transform = transforms.Compose([transforms.ToPILImage(), transforms.ToTensor()])\n",
    "\n",
    "class denoisingDataset(Dataset):\n",
    "    def __init__(self, data_groundTruth ,data_noisyImage, transform = None):\n",
    "        self.transform = transform\n",
    "        self.imgs_data       = data_groundTruth\n",
    "        self.noisy_imgs_data = data_noisyImage\n",
    "        \n",
    "    \n",
    "    def __getitem__(self, index):  \n",
    "        \n",
    "        img       = self.imgs_data[index,:,:,:]\n",
    "        noisy_img = self.noisy_imgs_data[index,:,:,:]\n",
    "\n",
    "        if self.transform is not None:            \n",
    "            img = self.transform(img)             \n",
    "            noisy_img = self.transform(noisy_img)  \n",
    "\n",
    "        return img, noisy_img\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imgs_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "10423e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "train_dataset       = denoisingDataset(train_groundTruth,train_noisyImage, transform = transform)\n",
    "valid_dataset       = denoisingDataset(test_groundTruth,valid_noisyImage, transform = transform)\n",
    "test_dataset       = denoisingDataset(valid_groundTruth,test_noisyImage, transform = transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "de43006e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=4,shuffle=True, num_workers=0)\n",
    "validloader = torch.utils.data.DataLoader(train_dataset, batch_size=4,shuffle=True, num_workers=0)\n",
    "testloader = torch.utils.data.DataLoader(train_dataset, batch_size=4,shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "965d54bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch\n",
    "\n",
    "def train_one_epoch(epoch_index, tb_writer):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    # Here, we use enumerate(training_loader) instead of\n",
    "    # iter(training_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "    for i, data in enumerate(trainloader):\n",
    "        # Every data instance is an input + label pair\n",
    "        inputs, trainGroundTruth = data\n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        outputs = unet(inputs)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        loss = criterion(outputs, trainGroundTruth)\n",
    "        loss.backward()\n",
    "\n",
    "        # Actualizacion de pesos\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if i % 10 == 9:\n",
    "            last_loss = running_loss / 10 # loss per batch\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            tb_x = epoch_index * len(trainloader) + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            running_loss = 0.\n",
    "\n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62c7b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "  batch 10 loss: 1.8680997848510743\n",
      "  batch 20 loss: 1.3842426240444183\n",
      "  batch 30 loss: 1.4859347939491272\n",
      "  batch 40 loss: 1.4209369838237762\n",
      "  batch 50 loss: 1.2849078565835952\n",
      "  batch 60 loss: 1.924480402469635\n",
      "  batch 70 loss: 1.715177631378174\n",
      "  batch 80 loss: 1.3233276188373566\n",
      "  batch 90 loss: 1.331294333934784\n",
      "  batch 100 loss: 1.6754584789276123\n",
      "  batch 110 loss: 1.5937489032745362\n",
      "  batch 120 loss: 1.5057116627693177\n",
      "  batch 130 loss: 1.7787162899971007\n",
      "  batch 140 loss: 1.749681407213211\n",
      "  batch 150 loss: 1.234049129486084\n",
      "  batch 160 loss: 1.5591177105903626\n",
      "  batch 170 loss: 1.475886207818985\n",
      "  batch 180 loss: 1.56326921582222\n",
      "  batch 190 loss: 1.6132418811321259\n",
      "  batch 200 loss: 1.3376784861087798\n",
      "  batch 210 loss: 1.4320926427841187\n",
      "  batch 220 loss: 1.3655160427093507\n",
      "  batch 230 loss: 1.231603269279003\n",
      "  batch 240 loss: 1.6065552711486817\n",
      "  batch 250 loss: 1.7651351153850556\n",
      "  batch 260 loss: 1.5759612321853638\n",
      "  batch 270 loss: 1.4622745215892792\n",
      "  batch 280 loss: 1.867682808637619\n",
      "  batch 290 loss: 1.6449345171451568\n",
      "  batch 300 loss: 1.3181196302175522\n",
      "  batch 310 loss: 1.4317071914672852\n",
      "  batch 320 loss: 1.7225321531295776\n",
      "  batch 330 loss: 1.353409993648529\n",
      "  batch 340 loss: 1.6529116123914718\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('runs/fashion_trainer_{}'.format(timestamp))\n",
    "epoch_number = 0\n",
    "\n",
    "EPOCHS = 1\n",
    "\n",
    "best_vloss = 1_000_000.\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    unet.train(True)\n",
    "    avg_loss = train_one_epoch(epoch_number, writer)\n",
    "\n",
    "    # We don't need gradients on to do reporting\n",
    "    unet.train(False)\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    for i, vdata in enumerate(validloader):\n",
    "        vinputs, validGroundTruth = vdata\n",
    "        voutputs = unet(vinputs)\n",
    "        vloss = criterion(voutputs,  validGroundTruth)\n",
    "        running_vloss += vloss\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "    # Log the running loss averaged per batch\n",
    "    # for both training and validation\n",
    "    writer.add_scalars('Training vs. Validation Loss',\n",
    "                    { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "                    epoch_number + 1)\n",
    "    writer.flush()\n",
    "\n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = 'model_{}_{}'.format(timestamp, epoch_number)\n",
    "        torch.save(unet.state_dict(), model_path)\n",
    "\n",
    "    epoch_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c3c536",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
